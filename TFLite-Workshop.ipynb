{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"TFLite-Workshop.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"o5FQf_9oLv5T","colab_type":"code","outputId":"578265ba-0ed3-428f-9a5a-092bbf3a4700","executionInfo":{"status":"ok","timestamp":1574586481102,"user_tz":-120,"elapsed":1136,"user":{"displayName":"Ahmed Ghoneim","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGUYqyKuUaSdKg5qlw-gbKw4UU7elPKXgMIUc-=s64","userId":"13149882919014521807"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/Workshop')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /Workshop; to attempt to forcibly remount, call drive.mount(\"/Workshop\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"caF5os6SL9Du","colab_type":"code","outputId":"dc6a5faf-7a47-4781-b705-d536a7378720","executionInfo":{"status":"ok","timestamp":1574586584628,"user_tz":-120,"elapsed":796,"user":{"displayName":"Ahmed Ghoneim","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGUYqyKuUaSdKg5qlw-gbKw4UU7elPKXgMIUc-=s64","userId":"13149882919014521807"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /Workshop/My\\ Drive/GUC-Workshop/Colab\\ files/keras_layers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/Workshop/My Drive/GUC-Workshop/Colab files/keras_layers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kkkNtsKYLhax","colab_type":"text"},"source":["<h1><center>TFLite Workshop - Notebook</center></h1>  <img src=\"https://drive.google.com/uc?id=1cPjFfXVFXsMMEejD2_9DtycCa6JwhSe9\"><img src=\"https://drive.google.com/uc?id=1GWXdIzlF58YNaChPmahb9LojnqwrEyfi\"> "]},{"cell_type":"code","metadata":{"id":"oY-M1z2QrJe2","colab_type":"code","colab":{}},"source":["%%bash\n","export PYTHONPATH=/Workshop/My\\ Drive/GUC-Workshop/Colab\\ files/:$PYTHONPATH"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8TIZXYWLhaz","colab_type":"text"},"source":["## Impotant imports"]},{"cell_type":"code","metadata":{"id":"vF488Yq1Lha0","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from keras.models import load_model\n","from keras.preprocessing import image\n","from imageio import imread\n","from matplotlib import pyplot as plt\n","import difflib\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iqc3YyFhLha3","colab_type":"text"},"source":["<img src=\"https://drive.google.com/uc?id=1DxUv_4LLDWzxAk0NRMX53PI0IMay90Po\">"]},{"cell_type":"markdown","metadata":{"id":"2DHGidPoLha4","colab_type":"text"},"source":["# SSD: Single Shot MultiBox Detector\n","Keras code for SSD-300 https://arxiv.org/pdf/1512.02325.pdf\n"]},{"cell_type":"code","metadata":{"id":"ZbVQ9RZuLha5","colab_type":"code","colab":{}},"source":["# import sys\n","# sys.path.append('/Workshop/My Drive/GUC-Workshop/Colab files')\n","# print (sys.path)\n","\n","from __future__ import division\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n","from keras.regularizers import l2\n","import keras.backend as K\n","\n","from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n","\n","def ssd_300(image_size,\n","            n_classes,\n","            mode='training',\n","            l2_regularization=0.0005,\n","            min_scale=None,\n","            max_scale=None,\n","            scales=None,\n","            aspect_ratios_global=None,\n","            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n","                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                     [1.0, 2.0, 0.5],\n","                                     [1.0, 2.0, 0.5]],\n","            two_boxes_for_ar1=True,\n","            steps=[8, 16, 32, 64, 100, 300],\n","            offsets=None,\n","            clip_boxes=False,\n","            variances=[0.1, 0.1, 0.2, 0.2],\n","            coords='centroids',\n","            normalize_coords=True,\n","            subtract_mean=[123, 117, 104],\n","            divide_by_stddev=None,\n","            swap_channels=[2, 1, 0],\n","            confidence_thresh=0.01,\n","            iou_threshold=0.45,\n","            top_k=200,\n","            nms_max_output_size=400,\n","            return_predictor_sizes=False):\n","\n","    n_predictor_layers = 6 # The number of predictor conv layers in the network is 6 for the original SSD300.\n","    n_classes += 1 # Account for the background class.\n","    l2_reg = l2_regularization # Make the internal name shorter.\n","    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n","\n","    ############################################################################\n","\n","    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n","        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n","    if aspect_ratios_per_layer:\n","        if len(aspect_ratios_per_layer) != n_predictor_layers:\n","            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n","\n","    if (min_scale is None or max_scale is None) and scales is None:\n","        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n","    if scales:\n","        if len(scales) != n_predictor_layers+1:\n","            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n","    else: # If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`\n","        scales = np.linspace(min_scale, max_scale, n_predictor_layers+1)\n","\n","    if len(variances) != 4:\n","        raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n","    variances = np.array(variances)\n","    if np.any(variances <= 0):\n","        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n","\n","    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n","        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n","\n","    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n","        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n","\n","    ############################################################################\n","    # Compute the anchor box parameters.\n","    ############################################################################\n","\n","    # Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.\n","    if aspect_ratios_per_layer:\n","        aspect_ratios = aspect_ratios_per_layer\n","    else:\n","        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n","\n","    # Compute the number of boxes to be predicted per cell for each predictor layer.\n","    # We need this so that we know how many channels the predictor layers need to have.\n","    if aspect_ratios_per_layer:\n","        n_boxes = []\n","        for ar in aspect_ratios_per_layer:\n","            if (1 in ar) & two_boxes_for_ar1:\n","                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n","            else:\n","                n_boxes.append(len(ar))\n","    else: # If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer\n","        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n","            n_boxes = len(aspect_ratios_global) + 1\n","        else:\n","            n_boxes = len(aspect_ratios_global)\n","        n_boxes = [n_boxes] * n_predictor_layers\n","\n","    if steps is None:\n","        steps = [None] * n_predictor_layers\n","    if offsets is None:\n","        offsets = [None] * n_predictor_layers\n","\n","    ############################################################################\n","    # Define functions for the Lambda layers below.\n","    ############################################################################\n","\n","    def identity_layer(tensor):\n","        return tensor\n","\n","    def input_mean_normalization(tensor):\n","        return tensor - np.array(subtract_mean)\n","\n","    def input_stddev_normalization(tensor):\n","        return tensor / np.array(divide_by_stddev)\n","\n","    def input_channel_swap(tensor):\n","        if len(swap_channels) == 3:\n","            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]]], axis=-1)\n","        elif len(swap_channels) == 4:\n","            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]], tensor[...,swap_channels[3]]], axis=-1)\n","\n","    ############################################################################\n","    # Build the network.\n","    ############################################################################\n","\n","    x = Input(shape=(img_height, img_width, img_channels))\n","\n","    # The following identity layer is only needed so that the subsequent lambda layers can be optional.\n","    x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n","    if not (subtract_mean is None):\n","        x1 = Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n","    if not (divide_by_stddev is None):\n","        x1 = Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n","    if swap_channels:\n","        x1 = Lambda(input_channel_swap, output_shape=(img_height, img_width, img_channels), name='input_channel_swap')(x1)\n","\n","    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(x1)\n","    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_2')(conv1_1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n","\n","    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n","    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_2')(conv2_1)\n","    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n","\n","    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n","    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_2')(conv3_1)\n","    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_3')(conv3_2)\n","    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n","\n","    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n","    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_2')(conv4_1)\n","    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3')(conv4_2)\n","    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n","\n","    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n","    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_2')(conv5_1)\n","    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_3')(conv5_2)\n","    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n","\n","    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n","\n","    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n","\n","    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n","    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n","    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n","\n","    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n","    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n","    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n","\n","    conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n","    conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n","\n","    conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n","    conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n","\n","    # Feed conv4_3 into the L2 normalization layer\n","    conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n","\n","    ### Build the convolutional predictor layers on top of the base network\n","\n","    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n","    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n","    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n","    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n","    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n","    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n","    conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n","    conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n","    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n","    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n","    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n","    fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n","    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n","    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n","    conv8_2_mbox_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n","    conv9_2_mbox_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n","\n","    ### Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\n","\n","    # Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n","    conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n","                                             two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0], clip_boxes=clip_boxes,\n","                                             variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n","    fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n","                                    two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1], clip_boxes=clip_boxes,\n","                                    variances=variances, coords=coords, normalize_coords=normalize_coords, name='fc7_mbox_priorbox')(fc7_mbox_loc)\n","    conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n","                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2], clip_boxes=clip_boxes,\n","                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n","    conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n","                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3], clip_boxes=clip_boxes,\n","                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n","    conv8_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], aspect_ratios=aspect_ratios[4],\n","                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[4], this_offsets=offsets[4], clip_boxes=clip_boxes,\n","                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv8_2_mbox_priorbox')(conv8_2_mbox_loc)\n","    conv9_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], aspect_ratios=aspect_ratios[5],\n","                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[5], this_offsets=offsets[5], clip_boxes=clip_boxes,\n","                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv9_2_mbox_priorbox')(conv9_2_mbox_loc)\n","\n","    ### Reshape\n","\n","    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n","    # We want the classes isolated in the last axis to perform softmax on them\n","    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n","    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n","    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n","    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n","    conv8_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n","    conv9_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n","    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n","    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n","    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n","    fc7_mbox_loc_reshape = Reshape((-1, 4), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n","    conv6_2_mbox_loc_reshape = Reshape((-1, 4), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n","    conv7_2_mbox_loc_reshape = Reshape((-1, 4), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n","    conv8_2_mbox_loc_reshape = Reshape((-1, 4), name='conv8_2_mbox_loc_reshape')(conv8_2_mbox_loc)\n","    conv9_2_mbox_loc_reshape = Reshape((-1, 4), name='conv9_2_mbox_loc_reshape')(conv9_2_mbox_loc)\n","    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n","    conv4_3_norm_mbox_priorbox_reshape = Reshape((-1, 8), name='conv4_3_norm_mbox_priorbox_reshape')(conv4_3_norm_mbox_priorbox)\n","    fc7_mbox_priorbox_reshape = Reshape((-1, 8), name='fc7_mbox_priorbox_reshape')(fc7_mbox_priorbox)\n","    conv6_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv6_2_mbox_priorbox_reshape')(conv6_2_mbox_priorbox)\n","    conv7_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv7_2_mbox_priorbox_reshape')(conv7_2_mbox_priorbox)\n","    conv8_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv8_2_mbox_priorbox_reshape')(conv8_2_mbox_priorbox)\n","    conv9_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv9_2_mbox_priorbox_reshape')(conv9_2_mbox_priorbox)\n","\n","    ### Concatenate the predictions from the different layers\n","\n","    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n","    # so we want to concatenate along axis 1, the number of boxes per layer\n","    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n","    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n","                                                       fc7_mbox_conf_reshape,\n","                                                       conv6_2_mbox_conf_reshape,\n","                                                       conv7_2_mbox_conf_reshape,\n","                                                       conv8_2_mbox_conf_reshape,\n","                                                       conv9_2_mbox_conf_reshape])\n","\n","    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n","    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n","                                                     fc7_mbox_loc_reshape,\n","                                                     conv6_2_mbox_loc_reshape,\n","                                                     conv7_2_mbox_loc_reshape,\n","                                                     conv8_2_mbox_loc_reshape,\n","                                                     conv9_2_mbox_loc_reshape])\n","\n","    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n","    mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n","                                                               fc7_mbox_priorbox_reshape,\n","                                                               conv6_2_mbox_priorbox_reshape,\n","                                                               conv7_2_mbox_priorbox_reshape,\n","                                                               conv8_2_mbox_priorbox_reshape,\n","                                                               conv9_2_mbox_priorbox_reshape])\n","\n","    # The box coordinate predictions will go into the loss function just the way they are,\n","    # but for the class predictions, we'll apply a softmax activation layer first\n","    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n","\n","    # Concatenate the class and box predictions and the anchors to one large predictions vector\n","    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n","    predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n","\n","    if mode == 'training':\n","        model = Model(inputs=x, outputs=predictions)\n","    elif mode == 'inference':\n","        decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n","                                               iou_threshold=iou_threshold,\n","                                               top_k=top_k,\n","                                               nms_max_output_size=nms_max_output_size,\n","                                               coords=coords,\n","                                               normalize_coords=normalize_coords,\n","                                               img_height=img_height,\n","                                               img_width=img_width,\n","                                               name='decoded_predictions')(predictions)\n","        model = Model(inputs=x, outputs=decoded_predictions)\n","    elif mode == 'inference_fast':\n","        decoded_predictions = DecodeDetectionsFast(confidence_thresh=confidence_thresh,\n","                                                   iou_threshold=iou_threshold,\n","                                                   top_k=top_k,\n","                                                   nms_max_output_size=nms_max_output_size,\n","                                                   coords=coords,\n","                                                   normalize_coords=normalize_coords,\n","                                                   img_height=img_height,\n","                                                   img_width=img_width,\n","                                                   name='decoded_predictions')(predictions)\n","        model = Model(inputs=x, outputs=decoded_predictions)\n","    else:\n","        raise ValueError(\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\".format(mode))\n","\n","    if return_predictor_sizes:\n","        predictor_sizes = np.array([conv4_3_norm_mbox_conf._keras_shape[1:3],\n","                                     fc7_mbox_conf._keras_shape[1:3],\n","                                     conv6_2_mbox_conf._keras_shape[1:3],\n","                                     conv7_2_mbox_conf._keras_shape[1:3],\n","                                     conv8_2_mbox_conf._keras_shape[1:3],\n","                                     conv9_2_mbox_conf._keras_shape[1:3]])\n","        return model, predictor_sizes\n","    else:\n","        return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dtuQhctLha7","colab_type":"code","colab":{}},"source":["# Set the image size.\n","img_height = 300\n","img_width = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVyG4TusLha9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":358},"outputId":"6b6ebf30-5714-4ec4-f4dd-d0cce7bbb1d8","executionInfo":{"status":"error","timestamp":1574586955481,"user_tz":-120,"elapsed":872,"user":{"displayName":"Ahmed Ghoneim","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGUYqyKuUaSdKg5qlw-gbKw4UU7elPKXgMIUc-=s64","userId":"13149882919014521807"}}},"source":["# 1: Build the Keras model\n","\n","K.clear_session() # Clear previous models from memory.\n","\n","model = ssd_300(image_size=(img_height, img_width, 3),\n","                n_classes=20,\n","                mode='inference',\n","                l2_regularization=0.0005,\n","                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n","                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n","                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                         [1.0, 2.0, 0.5],\n","                                         [1.0, 2.0, 0.5]],\n","                two_boxes_for_ar1=True,\n","                steps=[8, 16, 32, 64, 100, 300],\n","                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n","                clip_boxes=False,\n","                variances=[0.1, 0.1, 0.2, 0.2],\n","                normalize_coords=True,\n","                subtract_mean=[123, 117, 104],\n","                swap_channels=[2, 1, 0],\n","                confidence_thresh=0.5,\n","                iou_threshold=0.45,\n","                top_k=200,\n","                nms_max_output_size=400)\n","print (model.summary())"],"execution_count":17,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b73be86e6d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 nms_max_output_size=400)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-e1f25cef132a>\u001b[0m in \u001b[0;36mssd_300\u001b[0;34m(image_size, n_classes, mode, l2_regularization, min_scale, max_scale, scales, aspect_ratios_global, aspect_ratios_per_layer, two_boxes_for_ar1, steps, offsets, clip_boxes, variances, coords, normalize_coords, subtract_mean, divide_by_stddev, swap_channels, confidence_thresh, iou_threshold, top_k, nms_max_output_size, return_predictor_sizes)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Feed conv4_3 into the L2 normalization layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mconv4_3_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL2Normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv4_3_norm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m### Build the convolutional predictor layers on top of the base network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Workshop/My Drive/GUC-Workshop/Colab files/keras_layers/keras_layer_L2Normalization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gamma_init, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'image_dim_ordering'"]}]},{"cell_type":"code","metadata":{"id":"dEFEQbFxLha_","colab_type":"code","colab":{}},"source":["# 2: Load the trained weights into the model.\n","\n","weights_path = 'weights/VGG_VOC0712_SSD_300x300_ft_iter_120000.h5'\n","\n","model.load_weights(weights_path, by_name=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7v8N9BhLhbB","colab_type":"text"},"source":["# Testing the model"]},{"cell_type":"code","metadata":{"id":"nyWj-tQiLhbC","colab_type":"code","colab":{}},"source":["orig_images = [] # Store the images here.\n","input_images = [] # Store resized versions of the images here.\n","\n","# We'll only load one image in this example.\n","img_path = 'examples/fish-bike.jpg'\n","\n","orig_images.append(imread(img_path))\n","img = image.load_img(img_path, target_size=(img_height, img_width))\n","img = image.img_to_array(img) \n","input_images.append(img)\n","input_images = np.array(input_images)\n","\n","y_pred = model.predict(input_images)\n","\n","confidence_threshold = 0.5\n","\n","y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n","\n","np.set_printoptions(precision=2, suppress=True, linewidth=90)\n","print(\"Predicted boxes:\\n\")\n","print('   class   conf xmin   ymin   xmax   ymax')\n","print(y_pred_thresh[0])\n","\n","# Display the image and draw the predicted boxes onto it.\n","\n","# Set the colors for the bounding boxes\n","colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n","classes = ['background',\n","           'aeroplane', 'bicycle', 'bird', 'boat',\n","           'bottle', 'bus', 'car', 'cat',\n","           'chair', 'cow', 'diningtable', 'dog',\n","           'horse', 'motorbike', 'person', 'pottedplant',\n","           'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","plt.figure(figsize=(20,12))\n","plt.imshow(orig_images[0])\n","\n","current_axis = plt.gca()\n","\n","for box in y_pred_thresh[0]:\n","    # Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n","    xmin = box[2] * orig_images[0].shape[1] / img_width\n","    ymin = box[3] * orig_images[0].shape[0] / img_height\n","    xmax = box[4] * orig_images[0].shape[1] / img_width\n","    ymax = box[5] * orig_images[0].shape[0] / img_height\n","    color = colors[int(box[0])]\n","    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n","    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n","    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH2ULWExLhbE","colab_type":"text"},"source":["# Freezing models and exporting to protobuf (.pb) files"]},{"cell_type":"code","metadata":{"id":"1dRT-74_LhbF","colab_type":"code","colab":{}},"source":["def freeze_tf_model(model, out_file_name):\n","    \n","    K.set_learning_phase(0)\n","\n","    out_tensor_names = [out_tensor.name.split(':')[0] for out_tensor in model.outputs]\n","    \n","\n","    graph_def = K.get_session().graph.as_graph_def()\n","    \n","    # Replace all the variables in the graph with constants\n","    output_graph_def = tf.graph_util.convert_variables_to_constants(K.get_session(), graph_def, out_tensor_names)\n","\n","    # Serialize and dump the output graph to the filesystem\n","    with tf.gfile.GFile(out_file_name, 'wb') as f:\n","        f.write(output_graph_def.SerializeToString())\n","        \n","freeze_tf_model(model, 'frozen_ssd_model.pb')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iziqu5vALhbG","colab_type":"text"},"source":["# Converting to TensorFlowLite (.tflite)\n","### TFLite (float inference), but why?"]},{"cell_type":"code","metadata":{"id":"5xxaNAIOLhbH","colab_type":"code","colab":{}},"source":["def tflite_convertor(model_pb, out_file_name, model):\n","    converter = tf.lite.TFLiteConverter.from_frozen_graph(model_pb, \n","                                                          input_arrays=[model.input.name[:-2]], \n","                                                          output_arrays=[model.output.name[:-2]])\n","    converter.inference_type = tf.float32\n","    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","    tflite_serializable_quant = converter.convert()\n","    with open(out_file_name, 'wb') as f:\n","        f.write(tflite_serializable_quant)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-ZABAEFLhbJ","colab_type":"code","colab":{}},"source":["# Trial 1\n","tflite_convertor('frozen_ssd_model.pb', 'ssd_model.tflite', model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqV65tONLhbL","colab_type":"code","colab":{}},"source":["def print_changed_lines(f1, f2):\n","    text1 = open(f1).readlines()\n","    text2 = open(f2).readlines()\n","    for line in difflib.unified_diff(text1, text2):\n","        print (line)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSMzWOI3LhbN","colab_type":"code","colab":{}},"source":["print_changed_lines(\"keras_layers/keras_layer_DecodeDetections.py\", \"keras_layers/keras_layer_DecodeDetections1.py\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJFYhb9GLhbO","colab_type":"text"},"source":["# TensorFlow Graph Vs Python code"]},{"cell_type":"code","metadata":{"id":"FDUnMOmhLhbP","colab_type":"code","colab":{}},"source":["# Trial 2\n","tflite_convertor('frozen_ssd_model.pb', 'ssd_model.tflite', model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTKNTcXrLhbR","colab_type":"code","colab":{}},"source":["print_changed_lines(\"keras_layers/keras_layer_DecodeDetections1.py\", \"keras_layers/keras_layer_DecodeDetections2.py\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHiW8nJnLhbT","colab_type":"code","colab":{}},"source":["# Trial 3\n","tflite_convertor('frozen_ssd_model.pb', 'ssd_model.tflite', model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XpCm1N11LhbV","colab_type":"text"},"source":["<h1><center>Quantize-aware training</center></h1>\n","<h1><center>Quantized Vs Non-Qunatized models<center><h1>\n","    \n","<table><tr><td><img src='images/nonquant.png' width=\"500\" height=\"2000\"></td><td><img src='images/quant.png' width=\"500\" height=\"2000\"></td></tr></table>\n"]},{"cell_type":"markdown","metadata":{"id":"cQV8VgUqLhbV","colab_type":"text"},"source":["### TFLite (quantized)"]},{"cell_type":"code","metadata":{"id":"ryQVMi1WLhbW","colab_type":"code","colab":{}},"source":["%%bash\n","tflite_convert \\\n","--graph_def_file=ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/ssd_mobile_v2_quantized.pb \\\n","--output_file=detect.tflite \\\n","--input_shapes=1,300,300,3 \\\n","--input_arrays=normalized_input_image_tensor \\\n","--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 \\\n","--inference_type=QUANTIZED_UINT8 \\\n","--mean_values=0 \\\n","--std_dev_values=1 \\\n","--change_concat_input_ranges=false \\\n","--allow_custom_ops \n","\n"],"execution_count":0,"outputs":[]}]}